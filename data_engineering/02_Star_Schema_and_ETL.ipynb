{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RC Pakistan Cargo & Logistics - Star Schema and ETL Pipeline\n",
    "\n",
    "This notebook covers:\n",
    "1. Star schema design for logistics data warehouse\n",
    "2. Dimension table creation\n",
    "3. Fact table creation\n",
    "4. ETL pipeline implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"RC Pakistan Cargo & Logistics - Star Schema ETL Pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned datasets\n",
    "customers_df = pd.read_csv('../processed_data/customers_clean.csv')\n",
    "bookings_df = pd.read_csv('../processed_data/bookings_clean.csv')\n",
    "shipments_df = pd.read_csv('../processed_data/shipments_clean.csv')\n",
    "payments_df = pd.read_csv('../processed_data/payments_clean.csv')\n",
    "\n",
    "# Convert date columns\n",
    "customers_df['CreatedDate'] = pd.to_datetime(customers_df['CreatedDate'])\n",
    "bookings_df['BookingDate'] = pd.to_datetime(bookings_df['BookingDate'])\n",
    "shipments_df['ShipmentDate'] = pd.to_datetime(shipments_df['ShipmentDate'])\n",
    "shipments_df['ExpectedDelivery'] = pd.to_datetime(shipments_df['ExpectedDelivery'])\n",
    "payments_df['Date'] = pd.to_datetime(payments_df['Date'])\n",
    "\n",
    "print(\"Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DimDate - Time dimension\n",
    "def create_dim_date(start_date='2022-01-01', end_date='2022-12-31'):\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    \n",
    "    date_range = pd.date_range(start=start, end=end, freq='D')\n",
    "    \n",
    "    dim_date = pd.DataFrame({\n",
    "        'DateKey': [int(d.strftime('%Y%m%d')) for d in date_range],\n",
    "        'FullDate': date_range,\n",
    "        'Year': date_range.year,\n",
    "        'Quarter': date_range.quarter,\n",
    "        'Month': date_range.month,\n",
    "        'MonthName': date_range.strftime('%B'),\n",
    "        'Day': date_range.day,\n",
    "        'WeekDay': date_range.dayofweek + 1,\n",
    "        'WeekDayName': date_range.strftime('%A'),\n",
    "        'IsWeekend': (date_range.dayofweek >= 5).astype(int)\n",
    "    })\n",
    "    \n",
    "    return dim_date\n",
    "\n",
    "dim_date = create_dim_date()\n",
    "print(f\"DimDate created with {len(dim_date)} records\")\n",
    "print(dim_date.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DimCustomer\n",
    "dim_customer = customers_df.copy()\n",
    "dim_customer['CustomerKey'] = range(1, len(dim_customer) + 1)\n",
    "dim_customer = dim_customer[['CustomerKey', 'CustomerID', 'Name', 'Phone', 'City', 'CreatedDate']]\n",
    "dim_customer.rename(columns={'Name': 'CustomerName'}, inplace=True)\n",
    "\n",
    "print(f\"DimCustomer created with {len(dim_customer)} records\")\n",
    "print(dim_customer.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DimCity\n",
    "cities_data = [\n",
    "    (1, 'Dubai', 'UAE', 'Origin'),\n",
    "    (2, 'Sharjah', 'UAE', 'Origin'),\n",
    "    (3, 'Ajman', 'UAE', 'Origin'),\n",
    "    (4, 'Karachi', 'Pakistan', 'Destination'),\n",
    "    (5, 'Lahore', 'Pakistan', 'Destination'),\n",
    "    (6, 'Islamabad', 'Pakistan', 'Destination'),\n",
    "    (7, 'Rawalpindi', 'Pakistan', 'Destination'),\n",
    "    (8, 'Peshawar', 'Pakistan', 'Destination'),\n",
    "    (9, 'Mirpur', 'Azad Kashmir', 'Destination'),\n",
    "    (10, 'Muzaffarabad', 'Azad Kashmir', 'Destination')\n",
    "]\n",
    "\n",
    "dim_city = pd.DataFrame(cities_data, columns=['CityKey', 'CityName', 'Country', 'CityType'])\n",
    "print(f\"DimCity created with {len(dim_city)} records\")\n",
    "print(dim_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DimTransportMode\n",
    "transport_modes = [\n",
    "    (1, 'Air', 'Fast delivery, higher cost'),\n",
    "    (2, 'Sea', 'Economical, longer transit time')\n",
    "]\n",
    "\n",
    "dim_transport = pd.DataFrame(transport_modes, columns=['ModeKey', 'ModeName', 'Description'])\n",
    "print(f\"DimTransportMode created with {len(dim_transport)} records\")\n",
    "print(dim_transport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DimStatus\n",
    "status_data = [\n",
    "    (1, 'Booked', 'Initial booking created'),\n",
    "    (2, 'In Transit', 'Shipment in progress'),\n",
    "    (3, 'Arrived', 'Arrived at destination country'),\n",
    "    (4, 'Customs Cleared', 'Cleared customs procedures'),\n",
    "    (5, 'Delivered', 'Successfully delivered to customer')\n",
    "]\n",
    "\n",
    "dim_status = pd.DataFrame(status_data, columns=['StatusKey', 'StatusName', 'Description'])\n",
    "print(f\"DimStatus created with {len(dim_status)} records\")\n",
    "print(dim_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FactShipment - Main fact table\n",
    "def create_fact_shipment():\n",
    "    # Merge all related tables\n",
    "    fact_base = bookings_df.merge(shipments_df, on='BookingID', how='inner')\n",
    "    \n",
    "    # Create lookup dictionaries for dimension keys\n",
    "    customer_lookup = dict(zip(dim_customer['CustomerID'], dim_customer['CustomerKey']))\n",
    "    city_lookup = dict(zip(dim_city['CityName'], dim_city['CityKey']))\n",
    "    transport_lookup = {'Air': 1, 'Sea': 2}\n",
    "    status_lookup = {'Booked': 1, 'In Transit': 2, 'Arrived': 3, 'Customs Cleared': 4, 'Delivered': 5}\n",
    "    \n",
    "    # Create date keys\n",
    "    def get_date_key(date_col):\n",
    "        return pd.to_datetime(date_col).dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    fact_shipment = pd.DataFrame({\n",
    "        'ShipmentKey': range(1, len(fact_base) + 1),\n",
    "        'ShipmentID': fact_base['ShipmentID'],\n",
    "        'BookingID': fact_base['BookingID'],\n",
    "        'CustomerKey': fact_base['CustomerID'].map(customer_lookup),\n",
    "        'OriginCityKey': fact_base['Origin'].map(city_lookup),\n",
    "        'DestinationCityKey': fact_base['Destination'].map(city_lookup),\n",
    "        'TransportModeKey': fact_base['Mode'].map(transport_lookup),\n",
    "        'StatusKey': fact_base['Status_x'].map(status_lookup),\n",
    "        'BookingDateKey': get_date_key(fact_base['BookingDate']),\n",
    "        'ShipmentDateKey': get_date_key(fact_base['ShipmentDate']),\n",
    "        'ExpectedDeliveryDateKey': get_date_key(fact_base['ExpectedDelivery']),\n",
    "        'WeightKG': fact_base['WeightKG'],\n",
    "        'TransitDays': (pd.to_datetime(fact_base['ExpectedDelivery']) - pd.to_datetime(fact_base['ShipmentDate'])).dt.days\n",
    "    })\n",
    "    \n",
    "    return fact_shipment\n",
    "\n",
    "fact_shipment = create_fact_shipment()\n",
    "print(f\"FactShipment created with {len(fact_shipment)} records\")\n",
    "print(fact_shipment.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FactRevenue - Revenue analysis\n",
    "def create_fact_revenue():\n",
    "    # Merge payments with bookings for additional context\n",
    "    revenue_base = payments_df.merge(bookings_df, on='BookingID', how='inner')\n",
    "    \n",
    "    customer_lookup = dict(zip(dim_customer['CustomerID'], dim_customer['CustomerKey']))\n",
    "    \n",
    "    fact_revenue = pd.DataFrame({\n",
    "        'RevenueKey': range(1, len(revenue_base) + 1),\n",
    "        'PaymentID': revenue_base['PaymentID'],\n",
    "        'BookingID': revenue_base['BookingID'],\n",
    "        'CustomerKey': revenue_base['CustomerID'].map(customer_lookup),\n",
    "        'PaymentDateKey': pd.to_datetime(revenue_base['Date']).dt.strftime('%Y%m%d').astype(int),\n",
    "        'Amount': revenue_base['Amount'],\n",
    "        'PaymentMethod': revenue_base['Method'],\n",
    "        'WeightKG': revenue_base['WeightKG'],\n",
    "        'RevenuePerKG': revenue_base['Amount'] / revenue_base['WeightKG']\n",
    "    })\n",
    "    \n",
    "    return fact_revenue\n",
    "\n",
    "fact_revenue = create_fact_revenue()\n",
    "print(f\"FactRevenue created with {len(fact_revenue)} records\")\n",
    "print(fact_revenue.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Warehouse Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate star schema integrity\n",
    "print(\"Star Schema Validation:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check for orphaned records\n",
    "orphaned_customers = fact_shipment[~fact_shipment['CustomerKey'].isin(dim_customer['CustomerKey'])]\n",
    "print(f\"Orphaned customer records: {len(orphaned_customers)}\")\n",
    "\n",
    "orphaned_dates = fact_shipment[~fact_shipment['BookingDateKey'].isin(dim_date['DateKey'])]\n",
    "print(f\"Orphaned date records: {len(orphaned_dates)}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nStar Schema Summary:\")\n",
    "print(f\"Dimension Tables: 5\")\n",
    "print(f\"Fact Tables: 2\")\n",
    "print(f\"Total Records: {len(fact_shipment) + len(fact_revenue)}\")\n",
    "print(f\"Date Range: {dim_date['FullDate'].min()} to {dim_date['FullDate'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Star Schema to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLite database for the star schema\n",
    "conn = sqlite3.connect('../processed_data/rc_logistics_dw.db')\n",
    "\n",
    "# Save dimension tables\n",
    "dim_date.to_sql('DimDate', conn, if_exists='replace', index=False)\n",
    "dim_customer.to_sql('DimCustomer', conn, if_exists='replace', index=False)\n",
    "dim_city.to_sql('DimCity', conn, if_exists='replace', index=False)\n",
    "dim_transport.to_sql('DimTransportMode', conn, if_exists='replace', index=False)\n",
    "dim_status.to_sql('DimStatus', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Save fact tables\n",
    "fact_shipment.to_sql('FactShipment', conn, if_exists='replace', index=False)\n",
    "fact_revenue.to_sql('FactRevenue', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"Star schema saved to SQLite database: rc_logistics_dw.db\")\n",
    "print(\"ETL pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export for Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save star schema tables as CSV for further analysis\n",
    "import os\n",
    "os.makedirs('../star_schema', exist_ok=True)\n",
    "\n",
    "# Save dimension tables\n",
    "dim_date.to_csv('../star_schema/DimDate.csv', index=False)\n",
    "dim_customer.to_csv('../star_schema/DimCustomer.csv', index=False)\n",
    "dim_city.to_csv('../star_schema/DimCity.csv', index=False)\n",
    "dim_transport.to_csv('../star_schema/DimTransportMode.csv', index=False)\n",
    "dim_status.to_csv('../star_schema/DimStatus.csv', index=False)\n",
    "\n",
    "# Save fact tables\n",
    "fact_shipment.to_csv('../star_schema/FactShipment.csv', index=False)\n",
    "fact_revenue.to_csv('../star_schema/FactRevenue.csv', index=False)\n",
    "\n",
    "print(\"Star schema exported to CSV files in star_schema/ directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}